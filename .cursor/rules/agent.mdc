---
globs: agent/**/*.ts
alwaysApply: false
---
# LangGraph Agent Flow Documentation

## Flow Overview

The agent processes tasks through the following sequential stages:

1. **Start** → Index Codebase
2. **Index Codebase** → Setup Sandbox (on success) OR End (on failure)
3. **Setup Sandbox** → Plan Changes (on success) OR End (on failure)
4. **Plan Changes** → Edit Files (on success) OR End (on failure)
5. **Edit Files** → Create PR (on success) OR End (on failure)
6. **Create PR** → End

## Node Specifications

### 1. INDEX CODEBASE NODE

**Purpose**: Clone repository and create/update searchable index using Merkle tree for efficient incremental updates

**Inputs**:
- `task`: User task description
- `repoUrl`: Repository URL to clone
- `baseBranch`: Base branch to work from
- `previousMerkleRoot`: Previous Merkle tree root hash (from MySQL)

**Processing**:
1. Clone repository from repoUrl
2. **Merkle Tree Construction**:
   - Compute cryptographic hash for each valid code file
   - Build Merkle tree from file hashes (leaf nodes)
   - Combine hashes hierarchically to compute root hash
   - Store Merkle tree structure in MySQL for persistence
3. **Determine Indexing Scope**:
   - If no previous Merkle root exists → Index entire codebase
   - If previous root exists → Compare with current root
   - If roots match → Skip indexing (codebase unchanged)
   - If roots differ → Identify changed files via tree traversal
4. **Chunking Strategy** (for files requiring indexing):
   - Parse files using AST (Abstract Syntax Tree) for semantic boundaries
   - Split code into meaningful chunks (functions, classes, modules)
   - Respect token limits (e.g., 8192 for embeddings)
   - Preserve semantic context within chunks
5. **Embedding Generation**:
   - Generate embeddings using Vercel AI SDK
   - Create vector representations for each chunk
   - Include metadata: file path, line numbers, chunk hash
6. **Storage**:
   - Store embeddings in Qdrant vector database
   - Store Merkle tree and file hashes in MySQL
   - Cache embeddings indexed by chunk hash for reuse

**Outputs**:
- `indexedFiles`: Array of IndexedFile objects (only newly indexed/changed files)
- `merkleRoot`: Current Merkle tree root hash
- `changedFiles`: Array of file paths that changed since last index
- `isVectorDatabaseReady`: Boolean flag
- `currentStep`: 'indexing_complete' or 'indexing_failed'

**State Updates**:
- Populates indexedFiles with only changed/new files
- Updates merkleRoot in state and MySQL
- Sets isVectorDatabaseReady to true on success
- Stores Merkle tree structure for future comparisons
- Adds error to errors array on failure

**Incremental Update Benefits**:
- Avoids re-indexing unchanged files
- Reduces bandwidth and processing time
- Enables efficient caching of embeddings
- Supports team collaboration with shared index

---

### 2. SETUP SANDBOX NODE

**Purpose**: Create isolated Vercel Sandbox environment with Git-enabled repository access

**Inputs**:
- `repoUrl`: Repository URL (supports private repos)
- `baseBranch`: Base branch to checkout
- `indexedFiles`: Files from indexing stage
- GitHub App credentials (from env):
  - `GITHUB_APP_ID`
  - `GITHUB_APP_PRIVATE_KEY`
- Authentication: VERCEL_OIDC_TOKEN or (VERCEL_TOKEN + VERCEL_TEAM_ID + VERCEL_PROJECT_ID)

**Processing**:
1. Generate GitHub App installation access token
2. Initialize Vercel Sandbox with configuration:
   - Source: Git repository with authentication
   - Resources: vCPUs (1-8), memory allocation
   - Runtime: 'node22' or 'python3.13'
     - Node 22 runtime at `/vercel/runtimes/node22`
     - Python 3.13 runtime at `/vercel/runtimes/python`
   - Timeout: 5-45 minutes (default 5 minutes)
   - Ports: Array of ports to expose (e.g., [3000])
3. Clone repository into sandbox with GitHub App authentication:
   ```javascript
   source: {
     url: repoUrl,
     type: 'git',
     username: 'x-access-token',
     password: githubAppToken
   }
   ```
4. Configure Git identity for Pullsmith app:
   - Set git user.name to "Pullsmith[bot]"
   - Set git user.email to appropriate GitHub App email
5. Checkout the specified baseBranch
6. Install project dependencies if needed

**Outputs**:
- `sandboxId`: Unique identifier for sandbox instance
- `sandboxInstance`: Sandbox SDK instance for operations
- `isSandboxReady`: Boolean flag
- `currentStep`: 'sandbox_ready' or 'sandbox_failed'

**State Updates**:
- Sets sandboxId for subsequent file operations
- Stores sandboxInstance for command execution
- Sets isSandboxReady to true on success
- Adds error to errors array on failure

**Technical Details**:
- Sandbox runs on Amazon Linux 2023 base
- Default working directory: `/vercel/sandbox`
- User: `vercel-sandbox` with sudo access
- Sudo configuration:
  - HOME set to `/root` (sources root config files)
  - PATH unchanged (keeps local/project binaries)
  - Environment variables inherited
- Pre-installed packages include: git, openssl, tar, unzip, etc.
- Can install packages via `dnf` package manager
- Commands executed via `sandbox.runCommand()`:
  - Supports `detached: true` for long-running processes
  - Can stream stdout/stderr to process streams
- Public URLs accessible via `sandbox.domain(port)` method
- Sandbox can be stopped via `sandbox.stop()` method

---

### 3. PLAN CHANGES NODE

**Purpose**: Analyze task and create implementation plan using semantic search and LLM-based planning

**Inputs**:
- `task`: User task description
- `indexedFiles`: Indexed code files (from current session)
- `isVectorDatabaseReady`: Database ready flag
- `merkleRoot`: Current codebase state identifier

**Processing**:
1. **Enhanced Semantic Search**:
   - Query Qdrant vector database with task embedding
   - Retrieve top-10 most semantically similar code chunks
   - **Cross-file dependency analysis**: Include related files based on imports/exports
   - **Hierarchical context building**: Capture file relationships and dependencies
   - **Multi-modal search**: Combine semantic similarity with file structure analysis

2. **Context Assembly & Analysis**:
   - Collect relevant code chunks with metadata (file paths, line ranges)
   - **Dependency mapping**: Analyze imports, exports, and function calls
   - **Impact analysis**: Identify files that might be affected by changes
   - Build comprehensive context including:
     - Primary files (direct matches)
     - Related files (dependencies)
     - Configuration files (package.json, tsconfig.json, etc.)

3. **LLM-Based Plan Generation** (GitHub Copilot Workspace approach):
   - **Task-to-Spec-to-Plan workflow**:
     - Generate specification from task description
     - Create detailed implementation plan
     - Break down into prioritized action items
   - **Structured planning prompt**:
     ```
     Given task: {task}
     Codebase context: {semantic_matches}
     Dependencies: {file_relationships}
     
     Generate a structured plan with:
     1. Specification of changes needed
     2. File modification sequence (respecting dependencies)
     3. Specific actions per file (modify/create/delete)
     4. Validation steps
     ```
   - **Plan structure**:
     ```typescript
     interface PlanItem {
       action: 'modify' | 'create' | 'delete'
       filePath: string
       description: string
       priority: number
       dependencies: string[] // Other files this depends on
       changeType: 'implementation' | 'configuration' | 'test'
       estimatedComplexity: 'low' | 'medium' | 'high'
     }
     ```

4. **Dependency-Aware Prioritization**:
   - **Topological sorting**: Order changes based on dependencies
   - **Risk assessment**: Prioritize low-risk changes first
   - **Change impact scoring**: Estimate scope of each modification

**Outputs**:
- `relevantFiles`: Array of file paths relevant to task
- `semanticMatches`: Code chunks with similarity scores
- `plan`: Array of PlanItem objects with dependency-aware ordering
- `fileRelationships`: Map of file dependencies
- `currentStep`: 'planning_complete' or 'planning_failed'

**State Updates**:
- Populates relevantFiles with identified files
- Stores semantic search results for reference
- Creates detailed plan with prioritized actions
- Stores file relationship mapping
- Adds error to errors array on failure

**Implementation Strategy**:
- **Simple but effective**: Focus on clear plan generation over complex analysis
- **Dependency-aware**: Respect file relationships for correct ordering
- **Extensible**: Structure allows for future enhancements
- **Validation-ready**: Include validation steps in the plan

---

### 4. EDIT FILES NODE

**Purpose**: Execute planned changes using diff-based editing and validation in Vercel Sandbox

**Inputs**:
- `plan`: Array of PlanItem objects (dependency-ordered)
- `indexedFiles`: Original file contents
- `sandboxId`: Sandbox environment identifier
- `sandboxInstance`: Sandbox SDK instance
- `fileRelationships`: File dependency map

**Processing**:
1. **Dependency-Aware Execution**:
   - Sort plan items by priority and dependencies
   - Process items in topological order
   - Skip items if dependencies failed

2. **Diff-Based File Editing** (Research-backed approach):
   - **For file modifications**:
     - Read current file content from sandbox
     - **Generate targeted diff** using LLM with context:
       ```
       Original file: {current_content}
       Task: {plan_item.description}
       Context: {relevant_code_chunks}
       
       Generate minimal diff to implement the change:
       - Preserve existing structure
       - Make targeted modifications
       - Maintain code style consistency
       ```
     - **Apply diff strategically**:
       - Use line-based diff application
       - Preserve unchanged code regions
       - Handle conflicts gracefully
   
   - **For file creation**:
     - Generate complete file content with context
     - Use existing codebase patterns as templates
     - Include proper imports and exports
   
   - **For file deletion**:
     - Verify no remaining dependencies
     - Remove file and update references

3. **Iterative Validation & Refinement**:
   - **Syntax validation**:
     - Check syntax after each file modification
     - For TypeScript: Run `tsc --noEmit` for type checking
     - For JavaScript: Use ESLint for syntax validation
   
   - **Basic functionality validation**:
     - Attempt to import/require modified files
     - Check for obvious runtime errors
   
   - **Iterative refinement** (if validation fails):
     - Capture error messages
     - Retry with error context up to 2 times
     - Prompt: "Previous attempt failed with error: {error}. Fix the issue:"

4. **Progressive Validation Strategy**:
   - **Per-file validation**: Validate each file after modification
   - **Incremental building**: Check compilation after each change
   - **Rollback capability**: Track changes for potential rollback
   - **Confidence scoring**: Assess likelihood of success

5. **Sandbox Operations**:
   - File operations via Sandbox SDK
   - Command execution with timeout handling
   - Stream output for long-running processes
   - Git staging after successful modifications

**Outputs**:
- `editedFiles`: Array of EditedFile objects with diff metadata
- `validationResults`: Results of syntax and basic checks
- `failedItems`: Plan items that couldn't be completed
- `isEditingComplete`: Boolean flag
- `currentStep`: 'editing_complete' or 'editing_failed'

**State Updates**:
- Populates editedFiles with original/new content and diff information
- Records validation results for each file
- Tracks failed items for potential retry
- Sets isEditingComplete to true on success
- Adds detailed error info to errors array on failure

**Implementation Patterns**:
- **Diff-first approach**: Generate diffs rather than full file rewrites
- **Context-aware editing**: Use surrounding code for consistency
- **Fail-fast validation**: Stop early on critical errors
- **Graceful degradation**: Continue with other files if one fails
- **Audit trail**: Maintain detailed logs of all changes

**Error Handling & Recovery**:
- **Syntax errors**: Retry with error context
- **Type errors**: Include type information in retry
- **Import errors**: Check and fix import statements
- **Dependency errors**: Verify dependency order and retry
- **Sandbox errors**: Handle timeout and connection issues

**Validation Commands by Language**:
- **TypeScript**: `tsc --noEmit --project .`
- **JavaScript**: `eslint --no-eslintrc --config basic {file}`
- **Python**: `python -m py_compile {file}`
- **Go**: `go build -o /dev/null {file}`
- **Rust**: `rustc --emit=dep-info {file}`

---

### 5. CREATE PR NODE

**Purpose**: Create git branch and pull request using GitHub App within sandbox

**Inputs**:
- `editedFiles`: Modified file contents (already in sandbox)
- `repoUrl`: Repository URL
- `baseBranch`: Base branch for PR
- `task`: Original task description
- `sandboxId`: Sandbox environment identifier
- `sandboxInstance`: Sandbox SDK instance
- GitHub App credentials (from env)

**Processing within Sandbox**:
1. Generate unique branch name (e.g., `pullsmith/feature-${timestamp}`)
2. Create and checkout new branch:
   ```bash
   git checkout -b pullsmith/feature-${timestamp}
   ```
3. Commit changes with Pullsmith attribution:
   ```bash
   git commit -m "feat: ${task_summary}" -m "Automated changes by Pullsmith based on task: ${task}"
   ```
4. Push branch to remote using GitHub App token:
   ```bash
   git push origin pullsmith/feature-${timestamp}
   ```
5. Create pull request via GitHub API:
   - Title: Clear summary of changes
   - Body: Detailed description including task context
   - Author: Pullsmith[bot]
   - Base: baseBranch
   - Head: new feature branch
6. Stop sandbox using `sandboxInstance.stop()`

**Outputs**:
- `branchName`: Created branch name
- `commitHash`: Git commit hash
- `prUrl`: Pull request URL
- `currentStep`: 'pr_created' or 'pr_failed'

**State Updates**:
- Sets branchName for reference
- Sets commitHash for tracking
- Sets prUrl for access
- Adds error to errors array on failure

**Git Configuration**:
- All commits authored by "Pullsmith[bot]"
- Uses GitHub App authentication for push operations
- PR created via GitHub API with App credentials

## Implementation Techniques & Patterns

### LLM Integration Patterns

**Structured Prompting for Code Generation**:
- **Context-aware prompts**: Include relevant code chunks and file structure
- **Constraint-based generation**: Specify style, patterns, and architectural constraints
- **Iterative refinement**: Use compiler feedback for improvement
- **Confidence assessment**: Evaluate generated code quality

**Prompt Templates**:
```typescript
// Planning prompt template
const PLANNING_PROMPT = `
Analyze the following task and codebase context:

Task: {task}
Relevant Code: {semantic_matches}
File Structure: {file_relationships}

Generate a structured implementation plan with:
1. High-level specification
2. File modification sequence
3. Dependency considerations
4. Validation strategy

Focus on minimal, targeted changes that preserve existing patterns.
`;

// Diff generation prompt template
const DIFF_PROMPT = `
Original file content:
{current_content}

Task: {description}
Context: {relevant_context}

Generate a minimal diff to implement the required changes:
- Preserve existing code structure
- Maintain consistent code style
- Include necessary imports/exports
- Handle edge cases appropriately

Return only the modified sections with clear before/after markers.
`;
```

### Validation & Quality Assurance

**Multi-Layer Validation Pipeline**:
1. **Syntax validation**: Language-specific syntax checking
2. **Type validation**: Static type checking for typed languages
3. **Import/dependency validation**: Verify all imports resolve
4. **Basic functionality validation**: Test core functionality
5. **Style consistency validation**: Maintain codebase conventions

**Validation Strategy by Language**:
- **TypeScript/JavaScript**: ESLint + TypeScript compiler + import analysis
- **Python**: AST parsing + PyFlakes + import validation
- **Go**: Go compiler + gofmt + import analysis
- **Rust**: Rustc + Clippy + Cargo check

**Error Recovery Patterns**:
- **Iterative refinement**: Retry with error context (max 2 attempts)
- **Fallback strategies**: Simpler implementations if complex ones fail
- **Partial success handling**: Continue with other files if one fails
- **Rollback capability**: Maintain state for potential rollback

### Semantic Search Enhancement

**Context Enrichment Strategies**:
- **Hierarchical search**: File-level → Function-level → Line-level
- **Dependency-aware search**: Include related files based on imports
- **Pattern-based search**: Identify and reuse existing code patterns
- **Cross-language search**: Handle polyglot codebases effectively

**Search Result Processing**:
```typescript
interface EnhancedSemanticMatch {
  content: string;
  filePath: string;
  lineStart: number;
  lineEnd: number;
  score: number;
  context: {
    surroundingCode: string;
    imports: string[];
    exports: string[];
    dependencies: string[];
  };
  metadata: {
    language: string;
    fileType: string;
    lastModified: Date;
    complexity: 'low' | 'medium' | 'high';
  };
}
```

### Dependency Management

**File Relationship Analysis**:
- **Static analysis**: Parse imports/exports for dependency graph
- **Dynamic analysis**: Runtime dependency detection where possible
- **Circular dependency detection**: Identify and handle cycles
- **Impact analysis**: Predict change effects across files

**Dependency Resolution Strategies**:
- **Topological sorting**: Order changes by dependency requirements
- **Incremental updates**: Process changes in dependency order
- **Conflict resolution**: Handle competing changes to shared dependencies
- **Rollback planning**: Maintain dependency state for rollback

### Error Handling & Recovery

**Comprehensive Error Handling**:
- **Syntax errors**: Parse error messages and suggest fixes
- **Type errors**: Include type information in error context
- **Runtime errors**: Capture and analyze execution failures
- **Integration errors**: Handle API, database, and service failures

**Recovery Strategies**:
- **Automatic retry**: Retry failed operations with enhanced context
- **Graceful degradation**: Continue with reduced functionality
- **Human-in-the-loop**: Surface complex errors for manual resolution
- **Learning from failures**: Improve future attempts based on error patterns

### Performance Optimization

**Caching Strategies**:
- **Semantic search caching**: Cache search results for similar queries
- **Plan caching**: Reuse plans for similar tasks
- **Validation caching**: Cache validation results for unchanged files
- **Dependency caching**: Cache dependency analysis results

**Resource Management**:
- **Sandbox lifecycle**: Efficient creation, usage, and cleanup
- **Memory management**: Handle large codebases efficiently
- **Concurrent processing**: Parallel processing where safe
- **Rate limiting**: Respect API and service limits

## Sandbox Lifecycle Management

### Initialization
- Sandboxes are created with specific resource allocations (vCPUs)
- Each sandbox has a unique ID and public URLs via `domain(port)`
- Authentication handled via OIDC tokens or manual credentials

### Runtime Management
- Commands can run attached or detached (`detached: true`)
- Output streams can be piped to process.stdout/stderr
- Long-running processes (dev servers) run in background
- File system operations available through SDK methods

### Cleanup
- Sandboxes automatically stop after timeout (5-45 minutes)
- Can be manually stopped via `sandboxInstance.stop()`
- All resources are cleaned up on termination

### Observability
- Monitor sandboxes via Vercel dashboard Observability tab
- View command history and sandbox URLs
- Track compute usage across projects
- Access logs and execution details

### Best Practices
- Always stop sandboxes when work is complete
- Use appropriate timeouts for expected workload
- Stream logs for debugging during development
- Monitor resource usage to optimize costs

## Error Handling

Each node can fail, which will:
- Update `currentStep` to the corresponding failed state
- Add descriptive error message to the `errors` array
- Terminate the flow and proceed to End

## State Transitions

The agent maintains state throughout the flow using the `AgentState` interface, with each node reading required inputs and updating relevant outputs. The `currentStep` field tracks progress and allows for resumption or debugging of failed flows.

## Merkle Tree Indexing Strategy

Inspired by [Cursor's codebase indexing approach](https://read.engineerscodex.com/p/how-cursor-indexes-codebases-fast), Pullsmith uses Merkle trees for efficient incremental indexing:

### How It Works

1. **Initial Indexing**:
   - Compute hash for each file in the repository
   - Build Merkle tree with file hashes as leaf nodes
   - Store root hash and tree structure in MySQL
   - Generate embeddings for all files using Vercel AI SDK
   - Store embeddings in Qdrant vector database

2. **Incremental Updates**:
   - On subsequent runs, compute new Merkle tree
   - Compare new root hash with stored root hash
   - If identical: Skip indexing (no changes)
   - If different: Traverse tree to find changed files
   - Only re-index and embed changed files

3. **Storage Architecture**:
   - **MySQL**: Stores Merkle tree structure, file hashes, and metadata
   - **Qdrant**: Stores embeddings with chunk metadata
   - **Cache**: Embeddings indexed by chunk hash for reuse

### Key Benefits

- **Efficiency**: Only changed files are re-indexed
- **Bandwidth Optimization**: Minimal data transfer
- **Team Collaboration**: Shared index across team members
- **Data Integrity**: Cryptographic verification of file states
- **Scalability**: Handles large codebases efficiently

## 12-Hour Implementation Strategy

### Phase 1: Core Planning Implementation (3 hours)
**Priority**: Critical - Replace mock planning with actual LLM planning

**Tasks**:
1. **Implement LLM-based planning** (2 hours):
   - Create structured planning prompt
   - Generate PlanItem objects with dependencies
   - Use semantic search results as context
   - Implement basic dependency ordering

2. **Enhance semantic search context** (1 hour):
   - Add file relationship analysis
   - Include import/export detection
   - Expand search to include related files

**Success Criteria**:
- Plan generation produces actionable, ordered tasks
- Dependencies are correctly identified and ordered
- Context includes relevant related files

### Phase 2: Diff-Based Editing (4 hours)
**Priority**: Critical - Replace mock editing with actual file modifications

**Tasks**:
1. **Implement diff-based file editing** (2.5 hours):
   - Create diff generation prompts
   - Implement targeted file modification
   - Handle file creation/deletion/modification cases
   - Apply changes via Sandbox SDK

2. **Add basic validation** (1.5 hours):
   - Implement syntax checking (TypeScript/JavaScript focus)
   - Add retry logic for failed edits
   - Basic import/export validation

**Success Criteria**:
- Files are actually modified in sandbox
- Syntax errors are caught and retried
- Changes preserve existing code structure

### Phase 3: Validation & Integration (3 hours)
**Priority**: High - Ensure code quality and system integration

**Tasks**:
1. **Implement validation pipeline** (2 hours):
   - Add TypeScript compiler checking
   - Implement ESLint validation
   - Create validation result tracking
   - Add rollback capability for failed changes

2. **Integration testing** (1 hour):
   - Test end-to-end flow
   - Verify sandbox operations
   - Test with real repositories

**Success Criteria**:
- Validation catches major issues
- System works end-to-end
- Errors are handled gracefully

### Phase 4: Polish & Deployment (2 hours)
**Priority**: Medium - Refinement and deployment preparation

**Tasks**:
1. **Error handling refinement** (1 hour):
   - Improve error messages
   - Add logging and monitoring
   - Handle edge cases

2. **Performance optimization** (1 hour):
   - Add basic caching
   - Optimize prompt efficiency
   - Reduce API calls where possible

**Success Criteria**:
- System handles errors gracefully
- Performance is acceptable for development use
- Ready for initial deployment

### Implementation Priorities (If Time Constraints)

**Must Have (MVP)**:
- LLM-based planning (replace mock)
- Diff-based file editing (replace mock)
- Basic syntax validation
- End-to-end workflow completion

**Should Have (Enhanced)**:
- Dependency analysis and ordering
- Retry logic for failed edits
- Multiple language support
- Comprehensive error handling

**Could Have (Polish)**:
- Advanced validation pipeline
- Performance optimizations
- Caching strategies
- Detailed logging and monitoring

**Won't Have (Future)**:
- Complex multi-agent architectures
- Advanced AST analysis
- Machine learning-based optimizations
- Comprehensive test generation

### Development Guidelines

**Simple but Effective Approach**:
- Focus on core functionality over advanced features
- Use straightforward implementations over complex optimizations
- Prioritize working end-to-end flow over perfect individual components
- Implement validation incrementally

**Risk Mitigation**:
- Test each phase before moving to next
- Maintain working state at each milestone
- Use feature flags for experimental features
- Keep rollback plans for major changes

**Success Metrics**:
- Planning generates actionable tasks (not mock data)
- Files are actually modified in sandbox
- Basic validation catches obvious errors
- End-to-end flow completes successfully
- System handles common failure cases

```mermaid
graph TD
    A["Start"] --> B["Index Codebase<br/>• Clone repository<br/>• Build Merkle tree<br/>• Generate embeddings<br/>• Store in Qdrant"]
    
    B --> C{"Indexing<br/>Successful?"}
    C -->|Success| D["Setup Sandbox<br/>• Create Vercel Sandbox<br/>• Configure Git identity<br/>• Install dependencies"]
    C -->|Failure| E["End<br/>(Indexing Failed)"]
    
    D --> F{"Sandbox<br/>Ready?"}
    F -->|Success| G["Plan Changes<br/>• Semantic search<br/>• Query embeddings<br/>• Generate plan<br/>• Create action items"]
    F -->|Failure| H["End<br/>(Sandbox Failed)"]
    
    G --> I{"Planning<br/>Complete?"}
    I -->|Success| J["Edit Files<br/>• Execute plan items<br/>• Modify/create files<br/>• Validate syntax<br/>• Stage changes"]
    I -->|Failure| K["End<br/>(Planning Failed)"]
    
    J --> L{"Editing<br/>Complete?"}
    L -->|Success| M["Create PR<br/>• Create branch<br/>• Commit changes<br/>• Push to remote<br/>• Create pull request<br/>• Stop sandbox"]
    L -->|Failure| N["End<br/>(Editing Failed)"]
    
    M --> O{"PR Created<br/>Successfully?"}
    O -->|Success| P["End<br/>(Success)"]
    O -->|Failure| Q["End<br/>(PR Failed)"]
    
    style A fill:#e1f5fe
    style P fill:#e8f5e8
    style E fill:#ffebee
    style H fill:#ffebee
    style K fill:#ffebee
    style N fill:#ffebee
    style Q fill:#ffebee
```